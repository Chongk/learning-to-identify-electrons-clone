(venv) âžœ learning-to-identify-electrons git:(main) python .\train\training.py
hl


 Training...
feature_hl_numLayers_5_units_149_lr_0.0010_dp_0.0019_epochs_100


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.3029 - val_loss: 0.2939 - lr: 0.0010
Epoch 2/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2919 - val_loss: 0.2904 - lr: 0.0010
Epoch 3/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2894 - val_loss: 0.2851 - lr: 0.0010
Epoch 4/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2884 - val_loss: 0.2880 - lr: 0.0010
Epoch 5/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2874 - val_loss: 0.2881 - lr: 0.0010
Epoch 6/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2867 - val_loss: 0.2836 - lr: 0.0010
Epoch 7/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2870 - val_loss: 0.2834 - lr: 0.0010
Epoch 8/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2859 - val_loss: 0.2835 - lr: 0.0010
Epoch 9/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2859 - val_loss: 0.2870 - lr: 0.0010
Epoch 10/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2853 - val_loss: 0.2834 - lr: 0.0010
Epoch 11/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2852 - val_loss: 0.2830 - lr: 0.0010
Epoch 12/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2841 - val_loss: 0.2838 - lr: 0.0010
Epoch 13/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2845 - val_loss: 0.2848 - lr: 0.0010
Epoch 14/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2845 - val_loss: 0.2838 - lr: 0.0010
Epoch 15/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2840 - val_loss: 0.2867 - lr: 0.0010
Epoch 16/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2842 - val_loss: 0.2824 - lr: 0.0010
Epoch 17/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2839 - val_loss: 0.2845 - lr: 0.0010
Epoch 18/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2834 - val_loss: 0.2826 - lr: 0.0010
Epoch 19/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2834 - val_loss: 0.2825 - lr: 0.0010
Epoch 20/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2826 - val_loss: 0.2835 - lr: 0.0010
Epoch 21/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2833 - val_loss: 0.2828 - lr: 0.0010
Epoch 22/100
1075/1075 [==============================] - 5s 4ms/step - loss: 0.2799 - val_loss: 0.2811 - lr: 3.0000e-04
Epoch 23/100
1075/1075 [==============================] - 5s 5ms/step - loss: 0.2797 - val_loss: 0.2806 - lr: 3.0000e-04
Epoch 24/100
1075/1075 [==============================] - 5s 4ms/step - loss: 0.2796 - val_loss: 0.2815 - lr: 3.0000e-04
Epoch 25/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2794 - val_loss: 0.2806 - lr: 3.0000e-04
Epoch 26/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2793 - val_loss: 0.2806 - lr: 3.0000e-04
Epoch 27/100
1075/1075 [==============================] - 5s 5ms/step - loss: 0.2792 - val_loss: 0.2811 - lr: 3.0000e-04
Epoch 28/100
1075/1075 [==============================] - 5s 4ms/step - loss: 0.2792 - val_loss: 0.2812 - lr: 3.0000e-04
Epoch 29/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2780 - val_loss: 0.2807 - lr: 9.0000e-05
Epoch 30/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2778 - val_loss: 0.2808 - lr: 9.0000e-05
Epoch 31/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2779 - val_loss: 0.2806 - lr: 9.0000e-05
Epoch 32/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2777 - val_loss: 0.2805 - lr: 9.0000e-05
Epoch 33/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2777 - val_loss: 0.2809 - lr: 9.0000e-05
Epoch 34/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2777 - val_loss: 0.2805 - lr: 9.0000e-05
Epoch 35/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2775 - val_loss: 0.2809 - lr: 9.0000e-05
Epoch 36/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2775 - val_loss: 0.2806 - lr: 9.0000e-05
Epoch 37/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2775 - val_loss: 0.2808 - lr: 9.0000e-05
Epoch 38/100
1075/1075 [==============================] - 4s 4ms/step - loss: 0.2771 - val_loss: 0.2806 - lr: 2.7000e-05
Epoch 39/100
1075/1075 [==============================] - 4s 3ms/step - loss: 0.2770 - val_loss: 0.2807 - lr: 2.7000e-05
Epoch 40/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2769 - val_loss: 0.2808 - lr: 2.7000e-05
Epoch 41/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2770 - val_loss: 0.2807 - lr: 2.7000e-05
Epoch 42/100
1075/1075 [==============================] - 3s 3ms/step - loss: 0.2768 - val_loss: 0.2809 - lr: 2.7000e-05
Calculating performance on test set
1344/1344 [==============================] - 1s 528us/step
AUC 0.9471618567659053
et_and_ht


 Training...
feature_et_and_ht_numLayers_2_units_146_lr_0.0001_dp_0.0000_epochs_100_numConvBlocks_3_filters_47


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 410s 380ms/step - loss: 0.3620 - val_loss: 0.3133 - lr: 1.0000e-04
Epoch 2/100
1075/1075 [==============================] - 421s 392ms/step - loss: 0.2604 - val_loss: 0.2682 - lr: 1.0000e-04
Epoch 3/100
1075/1075 [==============================] - 417s 388ms/step - loss: 0.2276 - val_loss: 0.2194 - lr: 1.0000e-04
Epoch 4/100
1075/1075 [==============================] - 425s 395ms/step - loss: 0.2170 - val_loss: 0.2197 - lr: 1.0000e-04
Epoch 5/100
1075/1075 [==============================] - 422s 392ms/step - loss: 0.2127 - val_loss: 0.2129 - lr: 1.0000e-04
Epoch 6/100
1075/1075 [==============================] - 419s 389ms/step - loss: 0.2105 - val_loss: 0.2132 - lr: 1.0000e-04
Epoch 7/100
1075/1075 [==============================] - 412s 383ms/step - loss: 0.2086 - val_loss: 0.2171 - lr: 1.0000e-04
Epoch 8/100
1075/1075 [==============================] - 406s 377ms/step - loss: 0.2100 - val_loss: 0.2146 - lr: 1.0000e-04
Epoch 9/100
1075/1075 [==============================] - 412s 384ms/step - loss: 0.2080 - val_loss: 0.2103 - lr: 1.0000e-04
Epoch 10/100
1075/1075 [==============================] - 423s 394ms/step - loss: 0.2072 - val_loss: 0.2169 - lr: 1.0000e-04
Epoch 11/100
1075/1075 [==============================] - 411s 383ms/step - loss: 0.2085 - val_loss: 0.2185 - lr: 1.0000e-04
Epoch 12/100
1075/1075 [==============================] - 417s 388ms/step - loss: 0.2080 - val_loss: 0.2163 - lr: 1.0000e-04
Epoch 13/100
1075/1075 [==============================] - 457s 426ms/step - loss: 0.2056 - val_loss: 0.2111 - lr: 1.0000e-04
Epoch 14/100
1075/1075 [==============================] - 665s 618ms/step - loss: 0.2052 - val_loss: 0.2111 - lr: 1.0000e-04
Epoch 15/100
1075/1075 [==============================] - 631s 587ms/step - loss: 0.2020 - val_loss: 0.2074 - lr: 3.0000e-05
Epoch 16/100
1075/1075 [==============================] - 402s 374ms/step - loss: 0.2017 - val_loss: 0.2074 - lr: 3.0000e-05
Epoch 17/100
1075/1075 [==============================] - 413s 384ms/step - loss: 0.2017 - val_loss: 0.2072 - lr: 3.0000e-05
Epoch 18/100
1075/1075 [==============================] - 404s 376ms/step - loss: 0.2016 - val_loss: 0.2077 - lr: 3.0000e-05
Epoch 19/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.2014 - val_loss: 0.2077 - lr: 3.0000e-05
Epoch 20/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.2012 - val_loss: 0.2078 - lr: 3.0000e-05
Epoch 21/100
1075/1075 [==============================] - 412s 383ms/step - loss: 0.2009 - val_loss: 0.2079 - lr: 3.0000e-05
Epoch 22/100
1075/1075 [==============================] - 404s 375ms/step - loss: 0.2012 - val_loss: 0.2066 - lr: 3.0000e-05
Epoch 23/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.2007 - val_loss: 0.2080 - lr: 3.0000e-05
Epoch 24/100
1075/1075 [==============================] - 405s 376ms/step - loss: 0.2006 - val_loss: 0.2074 - lr: 3.0000e-05
Epoch 25/100
1075/1075 [==============================] - 405s 377ms/step - loss: 0.2005 - val_loss: 0.2082 - lr: 3.0000e-05
Epoch 26/100
1075/1075 [==============================] - 402s 374ms/step - loss: 0.2003 - val_loss: 0.2069 - lr: 3.0000e-05
Epoch 27/100
1075/1075 [==============================] - 402s 374ms/step - loss: 0.2003 - val_loss: 0.2071 - lr: 3.0000e-05
Epoch 28/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1992 - val_loss: 0.2062 - lr: 9.0000e-06
Epoch 29/100
1075/1075 [==============================] - 402s 374ms/step - loss: 0.1990 - val_loss: 0.2063 - lr: 9.0000e-06
Epoch 30/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1990 - val_loss: 0.2062 - lr: 9.0000e-06
Epoch 31/100
1075/1075 [==============================] - 400s 372ms/step - loss: 0.1990 - val_loss: 0.2062 - lr: 9.0000e-06
Epoch 32/100
1075/1075 [==============================] - 400s 372ms/step - loss: 0.1988 - val_loss: 0.2063 - lr: 9.0000e-06
Epoch 33/100
1075/1075 [==============================] - 400s 373ms/step - loss: 0.1989 - val_loss: 0.2063 - lr: 9.0000e-06
Epoch 34/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1984 - val_loss: 0.2060 - lr: 2.7000e-06
Epoch 35/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1984 - val_loss: 0.2062 - lr: 2.7000e-06
Epoch 36/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1983 - val_loss: 0.2058 - lr: 2.7000e-06
Epoch 37/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1983 - val_loss: 0.2059 - lr: 2.7000e-06
Epoch 38/100
1075/1075 [==============================] - 402s 374ms/step - loss: 0.1984 - val_loss: 0.2060 - lr: 2.7000e-06
Epoch 39/100
1075/1075 [==============================] - 405s 377ms/step - loss: 0.1983 - val_loss: 0.2060 - lr: 2.7000e-06
Epoch 40/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1983 - val_loss: 0.2060 - lr: 2.7000e-06
Epoch 41/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1983 - val_loss: 0.2059 - lr: 2.7000e-06
Epoch 42/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1982 - val_loss: 0.2058 - lr: 8.1000e-07
Epoch 43/100
1075/1075 [==============================] - 400s 372ms/step - loss: 0.1981 - val_loss: 0.2058 - lr: 8.1000e-07
Epoch 44/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1981 - val_loss: 0.2058 - lr: 8.1000e-07
Epoch 45/100
1075/1075 [==============================] - 400s 373ms/step - loss: 0.1981 - val_loss: 0.2058 - lr: 8.1000e-07
Epoch 46/100
1075/1075 [==============================] - 401s 373ms/step - loss: 0.1981 - val_loss: 0.2058 - lr: 8.1000e-07
Calculating performance on test set
1344/1344 [==============================] - 29s 21ms/step
AUC 0.9737834122792594
et


 Training...
feature_et_numLayers_2_units_160_lr_0.0001_dp_0.0000_epochs_100_numConvBlocks_3_filters_117


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 655s 609ms/step - loss: 0.4810 - val_loss: 0.3954 - lr: 1.0000e-04
Epoch 2/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3873 - val_loss: 0.3731 - lr: 1.0000e-04
Epoch 3/100
1075/1075 [==============================] - 653s 608ms/step - loss: 0.3746 - val_loss: 0.3613 - lr: 1.0000e-04
Epoch 4/100
1075/1075 [==============================] - 659s 613ms/step - loss: 0.3703 - val_loss: 0.3633 - lr: 1.0000e-04
Epoch 5/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3618 - val_loss: 0.3572 - lr: 1.0000e-04
Epoch 6/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3619 - val_loss: 0.3610 - lr: 1.0000e-04
Epoch 7/100
1075/1075 [==============================] - 659s 613ms/step - loss: 0.3608 - val_loss: 0.3598 - lr: 1.0000e-04
Epoch 8/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3578 - val_loss: 0.3547 - lr: 1.0000e-04
Epoch 9/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3575 - val_loss: 0.3537 - lr: 1.0000e-04
Epoch 10/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3572 - val_loss: 0.3594 - lr: 1.0000e-04
Epoch 11/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3547 - val_loss: 0.3661 - lr: 1.0000e-04
Epoch 12/100
1075/1075 [==============================] - 656s 610ms/step - loss: 0.3603 - val_loss: 0.3583 - lr: 1.0000e-04
Epoch 13/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3529 - val_loss: 0.3660 - lr: 1.0000e-04
Epoch 14/100
1075/1075 [==============================] - 655s 609ms/step - loss: 0.3519 - val_loss: 0.3521 - lr: 1.0000e-04
Epoch 15/100
1075/1075 [==============================] - 656s 610ms/step - loss: 0.3507 - val_loss: 0.3522 - lr: 1.0000e-04
Epoch 16/100
1075/1075 [==============================] - 656s 610ms/step - loss: 0.3512 - val_loss: 0.3512 - lr: 1.0000e-04
Epoch 17/100
1075/1075 [==============================] - 656s 611ms/step - loss: 0.3506 - val_loss: 0.3509 - lr: 1.0000e-04
Epoch 18/100
1075/1075 [==============================] - 657s 611ms/step - loss: 0.3495 - val_loss: 0.3508 - lr: 1.0000e-04
Epoch 19/100
1075/1075 [==============================] - 657s 611ms/step - loss: 0.3503 - val_loss: 0.3520 - lr: 1.0000e-04
Epoch 20/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3491 - val_loss: 0.3556 - lr: 1.0000e-04
Epoch 21/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3481 - val_loss: 0.3509 - lr: 1.0000e-04
Epoch 22/100
1075/1075 [==============================] - 653s 608ms/step - loss: 0.3472 - val_loss: 0.3521 - lr: 1.0000e-04
Epoch 23/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3471 - val_loss: 0.3537 - lr: 1.0000e-04
Epoch 24/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3434 - val_loss: 0.3506 - lr: 3.0000e-05
Epoch 25/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3432 - val_loss: 0.3492 - lr: 3.0000e-05
Epoch 26/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3430 - val_loss: 0.3495 - lr: 3.0000e-05
Epoch 27/100
1075/1075 [==============================] - 653s 608ms/step - loss: 0.3426 - val_loss: 0.3492 - lr: 3.0000e-05
Epoch 28/100
1075/1075 [==============================] - 655s 609ms/step - loss: 0.3423 - val_loss: 0.3496 - lr: 3.0000e-05
Epoch 29/100
1075/1075 [==============================] - 666s 619ms/step - loss: 0.3424 - val_loss: 0.3493 - lr: 3.0000e-05
Epoch 30/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3420 - val_loss: 0.3505 - lr: 3.0000e-05
Epoch 31/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3406 - val_loss: 0.3493 - lr: 9.0000e-06
Epoch 32/100
1075/1075 [==============================] - 652s 607ms/step - loss: 0.3403 - val_loss: 0.3493 - lr: 9.0000e-06
Epoch 33/100
1075/1075 [==============================] - 652s 607ms/step - loss: 0.3402 - val_loss: 0.3490 - lr: 9.0000e-06
Epoch 34/100
1075/1075 [==============================] - 654s 609ms/step - loss: 0.3401 - val_loss: 0.3489 - lr: 9.0000e-06
Epoch 35/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3401 - val_loss: 0.3492 - lr: 9.0000e-06
Epoch 36/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3399 - val_loss: 0.3494 - lr: 9.0000e-06
Epoch 37/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3398 - val_loss: 0.3494 - lr: 9.0000e-06
Epoch 38/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3397 - val_loss: 0.3493 - lr: 9.0000e-06
Epoch 39/100
1075/1075 [==============================] - 654s 608ms/step - loss: 0.3397 - val_loss: 0.3492 - lr: 9.0000e-06
Epoch 40/100
1075/1075 [==============================] - 655s 609ms/step - loss: 0.3392 - val_loss: 0.3493 - lr: 2.7000e-06
Epoch 41/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3391 - val_loss: 0.3492 - lr: 2.7000e-06
Epoch 42/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3391 - val_loss: 0.3492 - lr: 2.7000e-06
Epoch 43/100
1075/1075 [==============================] - 653s 607ms/step - loss: 0.3390 - val_loss: 0.3494 - lr: 2.7000e-06
Epoch 44/100
1075/1075 [==============================] - 652s 607ms/step - loss: 0.3389 - val_loss: 0.3493 - lr: 2.7000e-06
Calculating performance on test set
1344/1344 [==============================] - 49s 37ms/step
AUC 0.9194472314697755
ht


 Training...
feature_ht_numLayers_2_units_84_lr_0.0100_dp_0.5000_epochs_100_numConvBlocks_2_filters_27


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 106s 98ms/step - loss: 0.6933 - val_loss: 0.6932 - lr: 0.0100
Epoch 2/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6933 - val_loss: 0.6932 - lr: 0.0100
Epoch 3/100
1075/1075 [==============================] - 106s 99ms/step - loss: 0.6932 - val_loss: 0.6935 - lr: 0.0100
Epoch 4/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6933 - lr: 0.0100
Epoch 5/100
1075/1075 [==============================] - 104s 97ms/step - loss: 0.6933 - val_loss: 0.6932 - lr: 0.0100
Epoch 6/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6933 - val_loss: 0.6932 - lr: 0.0100
Epoch 7/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 0.0030
Epoch 8/100
1075/1075 [==============================] - 105s 97ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 0.0030
Epoch 9/100
1075/1075 [==============================] - 106s 99ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 0.0030
Epoch 10/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 0.0030
Epoch 11/100
1075/1075 [==============================] - 104s 97ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 0.0030
Epoch 12/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 9.0000e-04
Epoch 13/100
1075/1075 [==============================] - 106s 99ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 9.0000e-04
Epoch 14/100
1075/1075 [==============================] - 105s 97ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 9.0000e-04
Epoch 15/100
1075/1075 [==============================] - 106s 98ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 9.0000e-04
Epoch 16/100
1075/1075 [==============================] - 104s 97ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 9.0000e-04
Epoch 17/100
1075/1075 [==============================] - 105s 97ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 2.7000e-04
Epoch 18/100
1075/1075 [==============================] - 105s 97ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 2.7000e-04
Epoch 19/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6932 - lr: 2.7000e-04
Epoch 20/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 2.7000e-04
Epoch 21/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6932 - val_loss: 0.6931 - lr: 2.7000e-04
Epoch 22/100
1075/1075 [==============================] - 105s 97ms/step - loss: 0.6931 - val_loss: 0.6931 - lr: 8.1000e-05
Epoch 23/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6931 - val_loss: 0.6931 - lr: 8.1000e-05
Epoch 24/100
1075/1075 [==============================] - 105s 98ms/step - loss: 0.6931 - val_loss: 0.6931 - lr: 8.1000e-05
Epoch 25/100
1075/1075 [==============================] - 106s 98ms/step - loss: 0.6931 - val_loss: 0.6931 - lr: 8.1000e-05
Calculating performance on test set
1344/1344 [==============================] - 8s 6ms/step
AUC 0.5
et_and_ht_and_hl


 Training...
feature_et_and_ht_and_hl_numLayers_2_units_154_lr_0.0001_dp_0.0000_epochs_100_numConvBlocks_3_filters_34


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 259s 241ms/step - loss: 0.3466 - val_loss: 0.2976 - lr: 1.0000e-04
Epoch 2/100
1075/1075 [==============================] - 258s 240ms/step - loss: 0.2836 - val_loss: 0.2795 - lr: 1.0000e-04
Epoch 3/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.2661 - val_loss: 0.2454 - lr: 1.0000e-04
Epoch 4/100
1075/1075 [==============================] - 257s 239ms/step - loss: 0.2249 - val_loss: 0.2219 - lr: 1.0000e-04
Epoch 5/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.2128 - val_loss: 0.2142 - lr: 1.0000e-04
Epoch 6/100
1075/1075 [==============================] - 263s 245ms/step - loss: 0.2090 - val_loss: 0.2204 - lr: 1.0000e-04
Epoch 7/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.2073 - val_loss: 0.2093 - lr: 1.0000e-04
Epoch 8/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.2063 - val_loss: 0.2088 - lr: 1.0000e-04
Epoch 9/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.2049 - val_loss: 0.2081 - lr: 1.0000e-04
Epoch 10/100
1075/1075 [==============================] - 260s 242ms/step - loss: 0.2046 - val_loss: 0.2076 - lr: 1.0000e-04
Epoch 11/100
1075/1075 [==============================] - 258s 240ms/step - loss: 0.2031 - val_loss: 0.2062 - lr: 1.0000e-04
Epoch 12/100
1075/1075 [==============================] - 260s 242ms/step - loss: 0.2025 - val_loss: 0.2050 - lr: 1.0000e-04
Epoch 13/100
1075/1075 [==============================] - 262s 244ms/step - loss: 0.2017 - val_loss: 0.2048 - lr: 1.0000e-04
Epoch 14/100
1075/1075 [==============================] - 258s 240ms/step - loss: 0.2009 - val_loss: 0.2064 - lr: 1.0000e-04
Epoch 15/100
1075/1075 [==============================] - 262s 244ms/step - loss: 0.2010 - val_loss: 0.2090 - lr: 1.0000e-04
Epoch 16/100
1075/1075 [==============================] - 257s 239ms/step - loss: 0.2005 - val_loss: 0.2149 - lr: 1.0000e-04
Epoch 17/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1998 - val_loss: 0.2059 - lr: 1.0000e-04
Epoch 18/100
1075/1075 [==============================] - 259s 241ms/step - loss: 0.1995 - val_loss: 0.2048 - lr: 1.0000e-04
Epoch 19/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1964 - val_loss: 0.2024 - lr: 3.0000e-05
Epoch 20/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1959 - val_loss: 0.2029 - lr: 3.0000e-05
Epoch 21/100
1075/1075 [==============================] - 264s 245ms/step - loss: 0.1956 - val_loss: 0.2026 - lr: 3.0000e-05
Epoch 22/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1955 - val_loss: 0.2032 - lr: 3.0000e-05
Epoch 23/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1953 - val_loss: 0.2024 - lr: 3.0000e-05
Epoch 24/100
1075/1075 [==============================] - 261s 242ms/step - loss: 0.1951 - val_loss: 0.2023 - lr: 3.0000e-05
Epoch 25/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1940 - val_loss: 0.2020 - lr: 9.0000e-06
Epoch 26/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1939 - val_loss: 0.2024 - lr: 9.0000e-06
Epoch 27/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1938 - val_loss: 0.2021 - lr: 9.0000e-06
Epoch 28/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1937 - val_loss: 0.2021 - lr: 9.0000e-06
Epoch 29/100
1075/1075 [==============================] - 260s 242ms/step - loss: 0.1936 - val_loss: 0.2021 - lr: 9.0000e-06
Epoch 30/100
1075/1075 [==============================] - 262s 244ms/step - loss: 0.1935 - val_loss: 0.2019 - lr: 9.0000e-06
Epoch 31/100
1075/1075 [==============================] - 258s 240ms/step - loss: 0.1934 - val_loss: 0.2021 - lr: 9.0000e-06
Epoch 32/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1934 - val_loss: 0.2023 - lr: 9.0000e-06
Epoch 33/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1934 - val_loss: 0.2017 - lr: 9.0000e-06
Epoch 34/100
1075/1075 [==============================] - 262s 244ms/step - loss: 0.1933 - val_loss: 0.2020 - lr: 9.0000e-06
Epoch 35/100
1075/1075 [==============================] - 257s 239ms/step - loss: 0.1932 - val_loss: 0.2020 - lr: 9.0000e-06
Epoch 36/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1932 - val_loss: 0.2023 - lr: 9.0000e-06
Epoch 37/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1932 - val_loss: 0.2024 - lr: 9.0000e-06
Epoch 38/100
1075/1075 [==============================] - 264s 246ms/step - loss: 0.1931 - val_loss: 0.2024 - lr: 9.0000e-06
Epoch 39/100
1075/1075 [==============================] - 263s 245ms/step - loss: 0.1926 - val_loss: 0.2019 - lr: 2.7000e-06
Epoch 40/100
1075/1075 [==============================] - 258s 240ms/step - loss: 0.1926 - val_loss: 0.2018 - lr: 2.7000e-06
Epoch 41/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1926 - val_loss: 0.2019 - lr: 2.7000e-06
Epoch 42/100
1075/1075 [==============================] - 260s 242ms/step - loss: 0.1926 - val_loss: 0.2018 - lr: 2.7000e-06
Epoch 43/100
1075/1075 [==============================] - 265s 246ms/step - loss: 0.1925 - val_loss: 0.2019 - lr: 2.7000e-06
Calculating performance on test set
1344/1344 [==============================] - 23s 17ms/step
AUC 0.9752412735932261
mass


 Training...
feature_mass_numLayers_3_units_10_lr_0.0100_dp_0.0000_epochs_100


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 1s 892us/step - loss: 0.5538 - val_loss: 0.5380 - lr: 0.0100
Epoch 2/100
1075/1075 [==============================] - 1s 796us/step - loss: 0.5410 - val_loss: 0.5408 - lr: 0.0100
Epoch 3/100
1075/1075 [==============================] - 1s 795us/step - loss: 0.5406 - val_loss: 0.5473 - lr: 0.0100
Epoch 4/100
1075/1075 [==============================] - 1s 780us/step - loss: 0.5405 - val_loss: 0.5402 - lr: 0.0100
Epoch 5/100
1075/1075 [==============================] - 1s 799us/step - loss: 0.5399 - val_loss: 0.5378 - lr: 0.0100
Epoch 6/100
1075/1075 [==============================] - 1s 775us/step - loss: 0.5402 - val_loss: 0.5428 - lr: 0.0100
Epoch 7/100
1075/1075 [==============================] - 1s 775us/step - loss: 0.5395 - val_loss: 0.5381 - lr: 0.0100
Epoch 8/100
1075/1075 [==============================] - 1s 791us/step - loss: 0.5398 - val_loss: 0.5529 - lr: 0.0100
Epoch 9/100
1075/1075 [==============================] - 1s 773us/step - loss: 0.5398 - val_loss: 0.5390 - lr: 0.0100
Epoch 10/100
1075/1075 [==============================] - 1s 794us/step - loss: 0.5392 - val_loss: 0.5381 - lr: 0.0100
Epoch 11/100
1075/1075 [==============================] - 1s 767us/step - loss: 0.5381 - val_loss: 0.5379 - lr: 0.0030
Epoch 12/100
1075/1075 [==============================] - 1s 787us/step - loss: 0.5381 - val_loss: 0.5377 - lr: 0.0030
Epoch 13/100
1075/1075 [==============================] - 1s 769us/step - loss: 0.5381 - val_loss: 0.5378 - lr: 0.0030
Epoch 14/100
1075/1075 [==============================] - 1s 787us/step - loss: 0.5381 - val_loss: 0.5376 - lr: 0.0030
Epoch 15/100
1075/1075 [==============================] - 1s 776us/step - loss: 0.5382 - val_loss: 0.5375 - lr: 0.0030
Epoch 16/100
1075/1075 [==============================] - 1s 783us/step - loss: 0.5382 - val_loss: 0.5374 - lr: 0.0030
Epoch 17/100
1075/1075 [==============================] - 1s 770us/step - loss: 0.5381 - val_loss: 0.5379 - lr: 0.0030
Epoch 18/100
1075/1075 [==============================] - 1s 771us/step - loss: 0.5382 - val_loss: 0.5396 - lr: 0.0030
Epoch 19/100
1075/1075 [==============================] - 1s 765us/step - loss: 0.5381 - val_loss: 0.5374 - lr: 0.0030
Epoch 20/100
1075/1075 [==============================] - 1s 765us/step - loss: 0.5380 - val_loss: 0.5382 - lr: 0.0030
Epoch 21/100
1075/1075 [==============================] - 1s 750us/step - loss: 0.5382 - val_loss: 0.5377 - lr: 0.0030
Epoch 22/100
1075/1075 [==============================] - 1s 766us/step - loss: 0.5377 - val_loss: 0.5381 - lr: 9.0000e-04
Epoch 23/100
1075/1075 [==============================] - 1s 777us/step - loss: 0.5377 - val_loss: 0.5373 - lr: 9.0000e-04
Epoch 24/100
1075/1075 [==============================] - 1s 765us/step - loss: 0.5377 - val_loss: 0.5377 - lr: 9.0000e-04
Epoch 25/100
1075/1075 [==============================] - 1s 781us/step - loss: 0.5377 - val_loss: 0.5377 - lr: 9.0000e-04
Epoch 26/100
1075/1075 [==============================] - 1s 773us/step - loss: 0.5376 - val_loss: 0.5381 - lr: 9.0000e-04
Epoch 27/100
1075/1075 [==============================] - 1s 769us/step - loss: 0.5376 - val_loss: 0.5373 - lr: 2.7000e-04
Epoch 28/100
1075/1075 [==============================] - 1s 765us/step - loss: 0.5375 - val_loss: 0.5373 - lr: 2.7000e-04
Epoch 29/100
1075/1075 [==============================] - 1s 788us/step - loss: 0.5375 - val_loss: 0.5375 - lr: 2.7000e-04
Epoch 30/100
1075/1075 [==============================] - 1s 772us/step - loss: 0.5376 - val_loss: 0.5375 - lr: 2.7000e-04
Epoch 31/100
1075/1075 [==============================] - 1s 765us/step - loss: 0.5375 - val_loss: 0.5376 - lr: 2.7000e-04
Epoch 32/100
1075/1075 [==============================] - 1s 788us/step - loss: 0.5375 - val_loss: 0.5374 - lr: 8.1000e-05
Epoch 33/100
1075/1075 [==============================] - 1s 764us/step - loss: 0.5375 - val_loss: 0.5373 - lr: 8.1000e-05
Epoch 34/100
1075/1075 [==============================] - 1s 758us/step - loss: 0.5375 - val_loss: 0.5374 - lr: 8.1000e-05
Epoch 35/100
1075/1075 [==============================] - 1s 767us/step - loss: 0.5375 - val_loss: 0.5374 - lr: 8.1000e-05
Epoch 36/100
1075/1075 [==============================] - 1s 773us/step - loss: 0.5375 - val_loss: 0.5374 - lr: 8.1000e-05
Epoch 37/100
1075/1075 [==============================] - 1s 787us/step - loss: 0.5375 - val_loss: 0.5373 - lr: 2.4300e-05
Calculating performance on test set
1344/1344 [==============================] - 1s 494us/step
AUC 0.7765814788723411
hl_and_mass


 Training...
feature_hl_and_mass_numLayers_3_units_109_lr_0.0013_dp_0.0000_epochs_100


C:\Users\sigma\works\learning-to-identify-electrons\venv\lib\site-packages\keras\optimizers\optimizer_v2\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
  super().__init__(name, **kwargs)
Epoch 1/100
1075/1075 [==============================] - 2s 1ms/step - loss: 0.2963 - val_loss: 0.2733 - lr: 0.0013
Epoch 2/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2770 - val_loss: 0.2738 - lr: 0.0013
Epoch 3/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2752 - val_loss: 0.2753 - lr: 0.0013
Epoch 4/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2734 - val_loss: 0.2706 - lr: 0.0013
Epoch 5/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2735 - val_loss: 0.2746 - lr: 0.0013
Epoch 6/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2722 - val_loss: 0.2706 - lr: 0.0013
Epoch 7/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2722 - val_loss: 0.2743 - lr: 0.0013
Epoch 8/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2713 - val_loss: 0.2684 - lr: 0.0013
Epoch 9/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2714 - val_loss: 0.2682 - lr: 0.0013
Epoch 10/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2706 - val_loss: 0.2727 - lr: 0.0013
Epoch 11/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2706 - val_loss: 0.2675 - lr: 0.0013
Epoch 12/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2698 - val_loss: 0.2695 - lr: 0.0013
Epoch 13/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2694 - val_loss: 0.2702 - lr: 0.0013
Epoch 14/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2697 - val_loss: 0.2672 - lr: 0.0013
Epoch 15/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2691 - val_loss: 0.2683 - lr: 0.0013
Epoch 16/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2689 - val_loss: 0.2691 - lr: 0.0013
Epoch 17/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2686 - val_loss: 0.2688 - lr: 0.0013
Epoch 18/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2687 - val_loss: 0.2665 - lr: 0.0013
Epoch 19/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2691 - val_loss: 0.2676 - lr: 0.0013
Epoch 20/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2673 - val_loss: 0.2687 - lr: 0.0013
Epoch 21/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2675 - val_loss: 0.2674 - lr: 0.0013
Epoch 22/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2674 - val_loss: 0.2671 - lr: 0.0013
Epoch 23/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2667 - val_loss: 0.2692 - lr: 0.0013
Epoch 24/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2636 - val_loss: 0.2638 - lr: 3.9000e-04
Epoch 25/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2630 - val_loss: 0.2641 - lr: 3.9000e-04
Epoch 26/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2629 - val_loss: 0.2639 - lr: 3.9000e-04
Epoch 27/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2625 - val_loss: 0.2646 - lr: 3.9000e-04
Epoch 28/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2625 - val_loss: 0.2642 - lr: 3.9000e-04
Epoch 29/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2622 - val_loss: 0.2641 - lr: 3.9000e-04
Epoch 30/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2612 - val_loss: 0.2631 - lr: 1.1700e-04
Epoch 31/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2610 - val_loss: 0.2633 - lr: 1.1700e-04
Epoch 32/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2609 - val_loss: 0.2631 - lr: 1.1700e-04
Epoch 33/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2609 - val_loss: 0.2631 - lr: 1.1700e-04
Epoch 34/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2608 - val_loss: 0.2631 - lr: 1.1700e-04
Epoch 35/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2607 - val_loss: 0.2630 - lr: 1.1700e-04
Epoch 36/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2602 - val_loss: 0.2630 - lr: 3.5100e-05
Epoch 37/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2602 - val_loss: 0.2631 - lr: 3.5100e-05
Epoch 38/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2602 - val_loss: 0.2631 - lr: 3.5100e-05
Epoch 39/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2602 - val_loss: 0.2631 - lr: 3.5100e-05
Epoch 40/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2601 - val_loss: 0.2629 - lr: 3.5100e-05
Epoch 41/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2601 - val_loss: 0.2629 - lr: 3.5100e-05
Epoch 42/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2600 - val_loss: 0.2629 - lr: 1.0530e-05
Epoch 43/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2600 - val_loss: 0.2629 - lr: 1.0530e-05
Epoch 44/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 1.0530e-05
Epoch 45/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 1.0530e-05
Epoch 46/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 1.0530e-05
Epoch 47/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 3.1590e-06
Epoch 48/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 3.1590e-06
Epoch 49/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 3.1590e-06
Epoch 50/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 3.1590e-06
Epoch 51/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 3.1590e-06
Epoch 52/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 9.4770e-07
Epoch 53/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2599 - val_loss: 0.2629 - lr: 9.4770e-07
Epoch 54/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 9.4770e-07
Epoch 55/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 9.4770e-07
Epoch 56/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 9.4770e-07
Epoch 57/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 2.8431e-07
Epoch 58/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 2.8431e-07
Epoch 59/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 2.8431e-07
Epoch 60/100
1075/1075 [==============================] - 1s 1ms/step - loss: 0.2598 - val_loss: 0.2629 - lr: 2.8431e-07
Calculating performance on test set
1344/1344 [==============================] - 1s 480us/step
AUC 0.9565656617501178
(venv) âžœ learning-to-identify-electrons git:(main) .\venv\Scripts\activate